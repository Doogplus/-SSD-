{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD --- 使用Gluon\n",
    "\n",
    "本项目我们将实现SSD算法来检测野生皮卡丘。\n",
    "\n",
    "![](../img/pikachu.png)\n",
    "\n",
    "\n",
    "## 数据集\n",
    "\n",
    "我们使用一个开源的皮卡丘3D模型，用其生成1000张不同角度和大小的照片。然后将其随机的放在背景图片里。我们将图片打包成`rec`文件，这是一个MXNet常用的二进制数据格式。我们可以使用MXNet下的[tools/im2rec.py](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py)来将图片打包。\n",
    "\n",
    "### 下载数据\n",
    "\n",
    "打包好的数据可以直接在网上下载："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "root_url = ('https://apache-mxnet.s3-accelerate.amazonaws.com/'\n",
    "            'gluon/dataset/pikachu/')\n",
    "data_dir = '../data/pikachu/'\n",
    "dataset = {'train.rec': 'e6bcb6ffba1ac04ff8a9b1115e650af56ee969c8',\n",
    "          'train.idx': 'dcf7318b2602c06428b9988470c731621716c393',\n",
    "          'val.rec': 'd6c33f799b4d058e82f2cb5bd9a976f69d72d520'}\n",
    "for k, v in dataset.items():\n",
    "    gluon.utils.download(root_url+k, data_dir+k, sha1_hash=v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据集\n",
    "\n",
    "我们使用`image.ImageDetIter`来读取数据。这是针对物体检测的迭代器，(Det表示Detection)。它跟`image.ImageIter`使用很类似。主要不同是它返回的标号不是单个图片标号，而是每个图片里所有物体的标号，以及其对用的边框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import image\n",
    "from mxnet import nd\n",
    "\n",
    "data_dir = '../data/pikachu/'\n",
    "data_shape = 256\n",
    "batch_size = 32\n",
    "rgb_mean = nd.array([123, 117, 104])\n",
    "\n",
    "def get_iterators(data_shape, batch_size):\n",
    "    class_names = ['pikachu']\n",
    "    num_class = len(class_names)\n",
    "    train_iter = image.ImageDetIter(\n",
    "        batch_size=batch_size,\n",
    "        data_shape=(3, data_shape, data_shape),\n",
    "        path_imgrec=data_dir+'train.rec',\n",
    "        path_imgidx=data_dir+'train.idx',\n",
    "        shuffle=True,\n",
    "        mean=True,\n",
    "        rand_crop=1,\n",
    "        min_object_covered=0.95,\n",
    "        max_attempts=200)\n",
    "    val_iter = image.ImageDetIter(\n",
    "        batch_size=batch_size,\n",
    "        data_shape=(3, data_shape, data_shape),\n",
    "        path_imgrec=data_dir+'val.rec',\n",
    "        shuffle=False,\n",
    "        mean=True)\n",
    "    return train_iter, val_iter, class_names, num_class\n",
    "\n",
    "train_data, test_data, class_names, num_class = get_iterators(\n",
    "    data_shape, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们读取一个批量。可以看到标号的形状是`batch_size x num_object_per_image x 5`。这里数据里每个图片里面只有一个标号。每个标号由长为5的数组表示，第一个元素是其对用物体的标号，其中`-1`表示非法物体，仅做填充使用。后面4个元素表示边框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "batch = train_data.next()\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图示数据\n",
    "\n",
    "我们画出几张图片和其对应的标号。可以看到比卡丘的角度大小位置在每张图图片都不一样。不过也注意到这个数据集是直接将二次元动漫皮卡丘跟三次元背景相结合。可能通过简单判断区域的色彩直方图就可以有效的区别是不是有我们要的物体。我们用这个简单数据集来演示SSD是如何工作的。实际中遇到的数据集通常会复杂很多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def box_to_rect(box, color, linewidth=3):\n",
    "    \"\"\"convert an anchor box to a matplotlib rectangle\"\"\"\n",
    "    box = box.asnumpy()\n",
    "    return plt.Rectangle(\n",
    "        (box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
    "        fill=False, edgecolor=color, linewidth=linewidth)\n",
    "\n",
    "_, figs = plt.subplots(3, 3, figsize=(6,6))\n",
    "for i in range(3):\n",
    "    for j in range(3):        \n",
    "        img, labels = batch.data[0][3*i+j], batch.label[0][3*i+j]        \n",
    "        img = img.transpose((1, 2, 0)) + rgb_mean\n",
    "        img = img.clip(0,255).asnumpy()/255\n",
    "        fig = figs[i][j]\n",
    "        fig.imshow(img)\n",
    "        for label in labels:\n",
    "            rect = box_to_rect(label[1:5]*data_shape,'red',2)\n",
    "            fig.add_patch(rect)                    \n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD模型\n",
    "\n",
    "### 锚框：默认的边界框\n",
    "\n",
    "因为边框可以出现在图片中的任何位置，并且可以有任意大小。为了简化计算，SSD跟Faster R-CNN一样使用一些默认的边界框，或者称之为锚框（anchor box），做为搜索起点。具体来说，对输入的每个像素，以其为中心采样数个有不同形状和不同比例的边界框。假设输入大小是 $w \\times h$，\n",
    "\n",
    "- 给定大小 $s\\in (0,1]$，那么生成的边界框形状是 $ws \\times hs$\n",
    "- 给定比例 $r > 0$，那么生成的边界框形状是 $w\\sqrt{r} \\times \\frac{h}{\\sqrt{r}}$\n",
    "\n",
    "在采样的时候我们提供 $n$ 个大小（`sizes`）和 $m$ 个比例（`ratios`）。为了计算简单这里不生成$nm$个锚框，而是$n+m-1$个。其中第 $i$ 个锚框使用\n",
    "\n",
    "- `sizes[i]`和`ratios[0]` 如果 $i\\le n$\n",
    "- `sizes[0]`和`ratios[i-n]` 如果 $i>n$\n",
    "\n",
    "我们可以使用`contribe.ndarray`里的`MultiBoxPrior`来采样锚框。这里锚框通过左下角和右上角两个点来确定，而且被标准化成了区间$[0,1]$的实数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.contrib.ndarray import MultiBoxPrior\n",
    "\n",
    "# shape: batch x channel x height x weight\n",
    "n = 40\n",
    "x = nd.random.uniform(shape=(1, 3, n, n))\n",
    "\n",
    "y = MultiBoxPrior(x, sizes=[.5,.25,.1], ratios=[1,2,.5])\n",
    "\n",
    "boxes = y.reshape((n, n, -1, 4))\n",
    "print(boxes.shape)\n",
    "# The first anchor box centered on (20, 20)\n",
    "# its format is (x_min, y_min, x_max, y_max)\n",
    "boxes[20, 20, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以画出以`(20,20)`为中心的所有锚框："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "colors = ['blue', 'green', 'red', 'black', 'magenta']\n",
    "\n",
    "plt.imshow(nd.ones((n, n, 3)).asnumpy())\n",
    "anchors = boxes[20, 20, :, :]\n",
    "for i in range(anchors.shape[0]):\n",
    "    plt.gca().add_patch(box_to_rect(anchors[i,:]*n, colors[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测物体类别\n",
    "\n",
    "对每一个锚框我们需要预测它是不是包含了我们感兴趣的物体，还是只是背景。这里我们使用一个$3\\times 3$的卷积层来做预测，加上`pad=1`使用它的输出和输入一样。同时输出的通道数是`num_anchors*(num_classes+1)`，每个通道对应一个锚框对某个类的置信度。假设输出是`Y`，那么对应输入中第$n$个样本的第$(i,j)$像素的置信值是在`Y[n,:,i,j]`里。具体来说，对于以`(i,j)`为中心的第`a`个锚框，\n",
    "\n",
    "- 通道 `a*(num_class+1)` 是其只包含背景的分数\n",
    "- 通道 `a*(num_class+1)+1+b` 是其包含第`b`个物体的分数\n",
    "\n",
    "我们定义个一个这样的类别分类器函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "def class_predictor(num_anchors, num_classes):\n",
    "    \"\"\"return a layer to predict classes\"\"\"\n",
    "    return nn.Conv2D(num_anchors * (num_classes + 1), 3, padding=1)\n",
    "\n",
    "cls_pred = class_predictor(5, 10)\n",
    "cls_pred.initialize()\n",
    "x = nd.zeros((2, 3, 20, 20))\n",
    "y = cls_pred(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测边界框\n",
    "\n",
    "因为真实的边界框可以是任意形状，我们需要预测如何从一个锚框变换成真正的边界框。这个变换可以由一个长为4的向量来描述。同上一样，我们用一个有`num_anchors * 4`通道的卷积。假设输出是Y，那么对应输入中第 $n$ 个样本的第 $(i,j)$\n",
    "像素为中心的锚框的转换在`Y[n,:,i,j]`里。具体来说，对于第`a`个锚框，它的变换在`a*4`到`a*4+3`通道里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "def box_predictor(num_anchors):\n",
    "    \"\"\"return a layer to predict delta locations\"\"\"\n",
    "    return nn.Conv2D(num_anchors * 4, 3, padding=1)\n",
    "\n",
    "box_pred = box_predictor(10)\n",
    "box_pred.initialize()\n",
    "x = nd.zeros((2, 3, 20, 20))\n",
    "y = box_pred(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 减半模块\n",
    "\n",
    "我们定义一个卷积块，它将输入特征的长宽减半，以此来获取多尺度的预测。它由两个`Conv-BatchNorm-Relu`组成，我们使用填充为1的$3\\times 3$卷积使得输入和输入有同样的长宽，然后再通过跨度为2的最大池化层将长宽减半。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "def down_sample(num_filters):\n",
    "    \"\"\"stack two Conv-BatchNorm-Relu blocks and then a pooling layer\n",
    "    to halve the feature size\"\"\"\n",
    "    out = nn.HybridSequential()\n",
    "    for _ in range(2):\n",
    "        out.add(nn.Conv2D(num_filters, 3, strides=1, padding=1))\n",
    "        out.add(nn.BatchNorm(in_channels=num_filters))\n",
    "        out.add(nn.Activation('relu'))\n",
    "    out.add(nn.MaxPool2D(2))    #strides默认为2\n",
    "    return out\n",
    "\n",
    "blk = down_sample(10)\n",
    "blk.initialize()\n",
    "x = nd.zeros((2, 3, 40, 20))\n",
    "y = blk(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并来自不同层的预测输出\n",
    "\n",
    "前面我们提到过SSD的一个重要性质是它会在多个层同时做预测。每个层由于长宽和锚框选择不一样，导致输出的数据形状会不一样。这里我们用物体类别预测作为样例，边框预测是类似的。\n",
    "\n",
    "我们首先创建一个特定大小的输入，然后对它输出类别预测。然后对输入减半，再输出类别预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "x = nd.zeros((2, 8, 20, 20))\n",
    "print('x:', x.shape)\n",
    "\n",
    "cls_pred1 = class_predictor(5, 10)\n",
    "cls_pred1.initialize()\n",
    "y1 = cls_pred1(x)\n",
    "print('Class prediction 1:', y1.shape)\n",
    "\n",
    "ds = down_sample(16)\n",
    "ds.initialize()\n",
    "x = ds(x)\n",
    "print('x:', x.shape)\n",
    "\n",
    "cls_pred2 = class_predictor(3, 10)\n",
    "cls_pred2.initialize()\n",
    "y2 = cls_pred2(x)\n",
    "print('Class prediction 2:', y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到`y1`和`y2`形状不同。为了之后处理简单，我们将不同层的输入合并成一个输出。首先我们将通道移到最后的维度，然后将其展成2D数组。因为第一个维度是样本个数，所以不同输出之间是不变，我们可以将所有输出在第二个维度上拼接起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "def flatten_prediction(pred):\n",
    "    return pred.transpose(axes=(0,2,3,1)).flatten()\n",
    "\n",
    "def concat_predictions(preds):\n",
    "    return nd.concat(*preds, dim=1)\n",
    "\n",
    "flat_y1 = flatten_prediction(y1)\n",
    "print('Flatten class prediction 1', flat_y1.shape)\n",
    "flat_y2 = flatten_prediction(y2)\n",
    "print('Flatten class prediction 2', flat_y2.shape)\n",
    "y = concat_predictions([flat_y1, flat_y2])\n",
    "print('Concat class predictions', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主体网络\n",
    "\n",
    "主体网络用来从原始像素抽取特征。通常前面介绍的用来图片分类的卷积神经网络，例如ResNet，都可以用来作为主体网络。这里为了示范，我们简单叠加几个减半模块作为主体网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "def body():\n",
    "    out = nn.HybridSequential()\n",
    "    for nfilters in [16, 32, 64]:\n",
    "        out.add(down_sample(nfilters))\n",
    "    return out\n",
    "\n",
    "bnet = body()\n",
    "bnet.initialize()\n",
    "x = nd.random.uniform(shape=(2,3,256,256))\n",
    "y = bnet(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个玩具SSD模型\n",
    "\n",
    "现在我们可以创建一个玩具SSD模型了。我们称之为玩具是因为这个网络不管是层数还是锚框个数都比较小，仅仅适合之后我们之后使用的一个小数据集。但这个模型不会影响我们介绍SSD。\n",
    "\n",
    "这个网络包含四块。主体网络，三个减半模块，以及五个物体类别和边框预测模块。其中预测分别应用在在主体网络输出，减半模块输入，和最后的全局池化层上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toy_ssd_model(num_anchors, num_classes):\n",
    "    downsamplers = nn.Sequential()\n",
    "    for _ in range(3):\n",
    "        downsamplers.add(down_sample(128))\n",
    "        \n",
    "    class_predictors = nn.Sequential()\n",
    "    box_predictors = nn.Sequential()    \n",
    "    for _ in range(5):\n",
    "        class_predictors.add(class_predictor(num_anchors, num_classes))\n",
    "        box_predictors.add(box_predictor(num_anchors))\n",
    "\n",
    "    model = nn.Sequential()\n",
    "    model.add(body(), downsamplers, class_predictors, box_predictors)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算预测\n",
    "\n",
    "给定模型和每层预测输出使用的锚框大小和形状，我们可以定义前向函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toy_ssd_forward(x, model, sizes, ratios, verbose=False):    \n",
    "    body, downsamplers, class_predictors, box_predictors = model\n",
    "    anchors, class_preds, box_preds = [], [], []\n",
    "    # feature extraction    \n",
    "    x = body(x)\n",
    "    for i in range(5):\n",
    "        # predict\n",
    "        anchors.append(MultiBoxPrior(\n",
    "            x, sizes=sizes[i], ratios=ratios[i]))\n",
    "        class_preds.append(\n",
    "            flatten_prediction(class_predictors[i](x)))\n",
    "        box_preds.append(\n",
    "            flatten_prediction(box_predictors[i](x)))\n",
    "        if verbose:\n",
    "            print('Predict scale', i, x.shape, 'with', \n",
    "                  anchors[-1].shape[1], 'anchors')\n",
    "        # down sample\n",
    "        if i < 3:\n",
    "            x = downsamplers[i](x)\n",
    "        elif i == 3:\n",
    "            x = nd.Pooling(\n",
    "                x, global_pool=True, pool_type='max', \n",
    "                kernel=(x.shape[2], x.shape[3]))\n",
    "    # concat date\n",
    "    return (concat_predictions(anchors),\n",
    "            concat_predictions(class_preds),\n",
    "            concat_predictions(box_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完整的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "class ToySSD(gluon.Block):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ToySSD, self).__init__(**kwargs)\n",
    "        # anchor box sizes and ratios for 5 feature scales\n",
    "        self.sizes = [[.2,.272], [.37,.447], [.54,.619], \n",
    "                      [.71,.79], [.88,.961]]\n",
    "        self.ratios = [[1,2,.5]]*5\n",
    "        self.num_classes = num_classes\n",
    "        self.verbose = verbose\n",
    "        num_anchors = len(self.sizes[0]) + len(self.ratios[0]) - 1\n",
    "        # use name_scope to guard the names\n",
    "        with self.name_scope():\n",
    "            self.model = toy_ssd_model(num_anchors, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        anchors, class_preds, box_preds = toy_ssd_forward(\n",
    "            x, self.model, self.sizes, self.ratios, \n",
    "            verbose=self.verbose)\n",
    "        # it is better to have class predictions reshaped for softmax computation       \n",
    "        class_preds = class_preds.reshape(shape=(0, -1, self.num_classes+1))\n",
    "        return anchors, class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看一下输入图片的形状是如何改变的，已经输出的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "net = ToySSD(num_classes=2, verbose=True)\n",
    "net.initialize()\n",
    "x = batch.data[0][0:1]\n",
    "print('Input:', x.shape)\n",
    "anchors, class_preds, box_preds = net(x)\n",
    "print('Output achors:', anchors.shape)\n",
    "print('Output class predictions:', class_preds.shape)\n",
    "print('Output box predictions:', box_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "之前的教程我们主要是关注分类。对于分类的预测结果和真实的标号，我们通过交叉熵来计算他们的差异。但物体检测里我们需要预测边框。这里我们先引入一个概率来描述两个边框的距离。\n",
    "\n",
    "### IoU：交集除并集\n",
    "\n",
    "我们知道判断两个集合的相似度最常用的衡量叫做Jaccard距离，给定集合 $A$ 和 $B$，它的定义是 \n",
    "\n",
    "$$J(A,B) = \\frac{|A\\cap B|}{| A \\cup B|}$$\n",
    "\n",
    "边框可以看成是像素的集合，我们可以类似的定义它。这个标准通常被称之为 Intersection over Union (IoU)。\n",
    "\n",
    "![](../img/iou.svg)\n",
    "\n",
    "大的值表示两个边框很相似，而小的值则表示不相似。\n",
    "\n",
    "### 损失函数\n",
    "\n",
    "虽然每张图片里面通常只有几个标注的边框，但SSD会生成大量的锚框。可以想象很多锚框都不会框住感兴趣的物体，就是说跟任何对应感兴趣物体的表框的IoU都小于某个阈值。这样就会产生大量的负类锚框，或者说对应标号为0的锚框。对于这类锚框有两点要考虑的：\n",
    "\n",
    "1. 边框预测的损失函数不应该包括负类锚框，因为它们并没有对应的真实边框\n",
    "1. 因为负类锚框数目可能远多于其他，我们可以只保留其中的一些。而且是保留那些目前预测最不确信它是负类的，就是对类0预测值排序，选取数值最小的哪一些困难的负类锚框。\n",
    "\n",
    "我们可以使用`MultiBoxTarget`来完成上面这两个操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxTarget\n",
    "def training_targets(anchors, class_preds, labels):\n",
    "    class_preds = class_preds.transpose(axes=(0,2,1))\n",
    "    return MultiBoxTarget(anchors, labels, class_preds)\n",
    "\n",
    "out = training_targets(anchors, class_preds, batch.label[0][0:1]) \n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它返回三个`NDArray`，分别是\n",
    "\n",
    "1. 预测的边框跟真实边框的偏移，大小是`batch_size x (num_anchors*4)`\n",
    "1. 用来遮掩不需要的负类锚框的掩码，大小跟上面一致\n",
    "1. 锚框的真实的标号，大小是`batch_size x num_anchors`\n",
    "\n",
    "我们可以计算这次只选中了多少个锚框进入损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "out[1].sum()/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以定义需要的损失函数了。\n",
    "\n",
    "对于分类问题，最常用的损失函数是之前一直使用的交叉熵。这里我们定义一个类似于交叉熵的损失，不同于交叉熵的定义 $\\log(p_j)$，这里 $j$ 是真实的类别，且 $p_j$ 是对于的预测概率。我们使用一个被称之为关注损失的函数，给定正的$\\gamma$和$\\alpha$，它的定义是\n",
    "\n",
    "$$ - \\alpha (1-p_j)^{\\gamma} \\log(p_j) $$\n",
    "\n",
    "下图我们演示不同$\\gamma$导致的变化。可以看到，增加$\\gamma$可以使得对正类预测值比较大时损失变小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def focal_loss(gamma, x):\n",
    "    return - (1-x)**gamma*np.log(x)\n",
    "\n",
    "x = np.arange(0.01, 1, .01)\n",
    "gammas = [0,.25,.5,1]\n",
    "for i,g in enumerate(gammas):\n",
    "    plt.plot(x, focal_loss(g,x), colors[i])\n",
    "\n",
    "plt.legend(['gamma='+str(g) for g in gammas])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个自定义的损失函数可以简单通过继承`gluon.loss.Loss`来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "class FocalLoss(gluon.loss.Loss):\n",
    "    def __init__(self, axis=-1, alpha=0.25, gamma=2, batch_axis=0, **kwargs):\n",
    "        super(FocalLoss, self).__init__(None, batch_axis, **kwargs)\n",
    "        self._axis = axis\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def hybrid_forward(self, F, output, label):\n",
    "        output = F.softmax(output)\n",
    "        pj = output.pick(label, axis=self._axis, keepdims=True)\n",
    "        loss = - self._alpha * ((1 - pj) ** self._gamma) * pj.log()\n",
    "        return loss.mean(axis=self._batch_axis, exclude=True)\n",
    "\n",
    "cls_loss = FocalLoss()\n",
    "cls_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于边框的预测是一个回归问题。通常可以选择平方损失函数（L2损失）$f(x)=x^2$。但这个损失对于比较大的误差的惩罚很高。我们可以采用稍微缓和一点绝对损失函数（L1损失）$f(x)=|x|$，它是随着误差线性增长，而不是平方增长。但这个函数在0点处倒是不唯一，因此可能会影响收敛。一个通常的解决办法是在0点附近使用平方函数使得它更加平滑。它被称之为平滑L1损失函数。它通过一个参数$\\sigma$来控制平滑的区域：\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "    \\begin{cases}\n",
    "    (\\sigma x)^2/2,& \\text{if }x < 1/\\sigma^2\\\\\n",
    "    |x|-0.5/\\sigma^2,& \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "我们图示不同的$\\sigma$的平滑L1损失和L2损失的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [.5, 1, 10]\n",
    "x = nd.arange(-2, 2, 0.1)\n",
    "\n",
    "for i,s in enumerate(scales):\n",
    "    y = nd.smooth_l1(x, scalar=s)\n",
    "    plt.plot(x.asnumpy(), y.asnumpy(), color=colors[i])\n",
    "plt.plot(x.asnumpy(), (x**2).asnumpy(), color=colors[len(scales)])\n",
    "plt.legend(['scale='+str(s) for s in scales]+['Square loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们同样通过继承`Loss`来定义这个损失。同时它接受一个额外参数`mask`，这是用来屏蔽掉不需要被惩罚的负例样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [],
   "source": [
    "class SmoothL1Loss(gluon.loss.Loss):\n",
    "    def __init__(self, batch_axis=0, **kwargs):\n",
    "        super(SmoothL1Loss, self).__init__(None, batch_axis, **kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, output, label, mask):\n",
    "        loss = F.smooth_l1((output - label) * mask, scalar=1.0)\n",
    "        return loss.mean(self._batch_axis, exclude=True)\n",
    "\n",
    "box_loss = SmoothL1Loss()\n",
    "box_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估测量\n",
    "\n",
    "对于分类好坏我们可以沿用之前的分类精度。评估边框预测的好坏的一个常用是是平均绝对误差。记得在[线性回归](../chapter_supervised-learning/linear-regression-scratch.md)我们使用了平均平方误差。但跟上面对损失函数的讨论一样，平方误差对于大的误差给予过大的值，从而数值上过于敏感。平均绝对误差就是将二次项替换成绝对值，具体来说就是预测的边框和真实边框在4个维度上的差值的绝对值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import metric\n",
    "\n",
    "cls_metric = metric.Accuracy()\n",
    "box_metric = metric.MAE() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型和训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "22"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import init\n",
    "from mxnet import gpu\n",
    "\n",
    "ctx = gpu(0)\n",
    "# the CUDA implementation requres each image has at least 3 lables. \n",
    "# Padd two -1 labels for each instance \n",
    "train_data.reshape(label_shape=(3, 5))\n",
    "train_data = test_data.sync_label_shape(train_data)\n",
    "\n",
    "net = ToySSD(num_class)\n",
    "net.initialize(init.Xavier(magnitude=2), ctx=ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), \n",
    "                        'sgd', {'learning_rate': 0.1, 'wd': 5e-4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "训练函数跟前面的不一样在于网络会有多个输出，而且有两个损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "25"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from mxnet import autograd\n",
    "for epoch in range(30):\n",
    "    # reset data iterators and metrics \n",
    "    train_data.reset()\n",
    "    cls_metric.reset()\n",
    "    box_metric.reset()\n",
    "    tic = time.time()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        x = batch.data[0].as_in_context(ctx)\n",
    "        y = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            anchors, class_preds, box_preds = net(x)\n",
    "            box_target, box_mask, cls_target = training_targets(\n",
    "                anchors, class_preds, y)\n",
    "            # losses\n",
    "            loss1 = cls_loss(class_preds, cls_target)\n",
    "            loss2 = box_loss(box_preds, box_target, box_mask)\n",
    "            loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        # update metrics\n",
    "        cls_metric.update([cls_target], [class_preds.transpose((0,2,1))])\n",
    "        box_metric.update([box_target], [box_preds * box_mask])\n",
    "\n",
    "    print('Epoch %2d, train %s %.2f, %s %.5f, time %.1f sec' % (\n",
    "        epoch, *cls_metric.get(), *box_metric.get(), time.time()-tic\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "\n",
    "在预测阶段，我们希望能把图片里面所有感兴趣的物体找出来。\n",
    "\n",
    "我们先定一个数据读取和预处理函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "26"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        im = image.imdecode(f.read())\n",
    "    # resize to data_shape\n",
    "    data = image.imresize(im, data_shape, data_shape)\n",
    "    # minus rgb mean\n",
    "    data = data.astype('float32') - rgb_mean\n",
    "    # convert to batch x channel x height xwidth\n",
    "    return data.transpose((2,0,1)).expand_dims(axis=0), im\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们跟训练那样预测表框和其对应的物体。但注意到因为我们对每个像素都会生成数个锚框，这样我们可能会预测出大量相似的表框，从而导致结果非常嘈杂。一个办法是对于IoU比较高的两个表框，我们只保留预测执行度比较高的那个。这个算法（称之为non maximum suppression）在`MultiBoxDetection`里实现了。下面我们实现预测函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "27"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxDetection\n",
    "\n",
    "def predict(x):\n",
    "    anchors, cls_preds, box_preds = net(x.as_in_context(ctx))\n",
    "    cls_probs = nd.SoftmaxActivation(\n",
    "        cls_preds.transpose((0,2,1)), mode='channel')\n",
    "\n",
    "    return MultiBoxDetection(cls_probs, box_preds, anchors,\n",
    "                             force_suppress=True, clip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测函数会输出所有边框，每个边框由`[class_id, confidence, xmin, ymin, xmax, ymax]`表示。其中`class_id=-1`表示要么这个边框被预测只含有背景，或者被去重掉了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, im = process_image('../img/pikachu.jpg')\n",
    "out = predict(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们将预测出置信度超过某个阈值的边框画出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "29"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "def display(im, out, threshold=0.5):    \n",
    "    plt.imshow(im.asnumpy())\n",
    "    for row in out:\n",
    "        row = row.asnumpy()\n",
    "        class_id, score = int(row[0]), row[1]\n",
    "        if class_id < 0 or score < threshold:\n",
    "            continue\n",
    "        color = colors[class_id%len(colors)]\n",
    "        box = row[2:6] * np.array([im.shape[0],im.shape[1]]*2)\n",
    "        rect = box_to_rect(nd.array(box), color, 2)\n",
    "        plt.gca().add_patch(rect)\n",
    "                        \n",
    "        text = class_names[class_id]\n",
    "        plt.gca().text(box[0], box[1], \n",
    "                       '{:s} {:.2f}'.format(text, score),\n",
    "                       bbox=dict(facecolor=color, alpha=0.5),\n",
    "                       fontsize=10, color='white')\n",
    "    plt.show()\n",
    "\n",
    "display(im, out[0], threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "\n",
    "物体检测比分类要困难很多。因为我们不仅要预测物体类别，还要找到它们的位置。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
